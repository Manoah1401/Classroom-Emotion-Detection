{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\noble\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detection model...\n",
      "Face cropping process is complete.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"[INFO] loading face detection model...\")\n",
    "prototxt_path = \"deploy.prototxt.txt\"  \n",
    "model_path = \"res10_300x300_ssd_iter_140000.caffemodel\"  \n",
    "face_net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "\n",
    "train_test_base_dir = 'train_test'  \n",
    "cropped_images_base_dir = 'cropped_images_1'  \n",
    "\n",
    "\n",
    "os.makedirs(cropped_images_base_dir, exist_ok=True)\n",
    "\n",
    "def detect_and_crop_faces(image_path, output_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not read image {image_path}\")\n",
    "        return\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    face_net.setInput(blob)\n",
    "    detections = face_net.forward()\n",
    "\n",
    "    faces_found = False  \n",
    "\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            if startX >= 0 and startY >= 0 and endX <= w and endY <= h:\n",
    "                face = image[startY:endY, startX:endX]\n",
    "                if face.size > 0:\n",
    "                    cv2.imwrite(output_path, face)\n",
    "                    faces_found = True\n",
    "                    break \n",
    "    if not faces_found:\n",
    "        print(f\"No valid faces found in image {image_path}\")\n",
    "\n",
    "for main_dir_name in os.listdir(train_test_base_dir):\n",
    "    main_dir_path = os.path.join(train_test_base_dir, main_dir_name)\n",
    "    if os.path.isdir(main_dir_path):\n",
    "        cropped_main_dir_path = os.path.join(cropped_images_base_dir, main_dir_name)\n",
    "        os.makedirs(cropped_main_dir_path, exist_ok=True)\n",
    "\n",
    "        for person_name in os.listdir(main_dir_path):\n",
    "            person_dir = os.path.join(main_dir_path, person_name)\n",
    "            if os.path.isdir(person_dir):\n",
    "                cropped_faces_person_dir = os.path.join(cropped_main_dir_path, person_name)\n",
    "                os.makedirs(cropped_faces_person_dir, exist_ok=True)\n",
    "\n",
    "                for image_file in os.listdir(person_dir):\n",
    "                    if image_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                        image_path = os.path.join(person_dir, image_file)\n",
    "                        output_path = os.path.join(cropped_faces_person_dir, image_file)  \n",
    "                        detect_and_crop_faces(image_path, output_path)\n",
    "\n",
    "print(\"Face cropping process is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              25691136  \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1024)              4096      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40415045 (154.17 MB)\n",
      "Trainable params: 25698309 (98.03 MB)\n",
      "Non-trainable params: 14716736 (56.14 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.BatchNormalization())  \n",
    "model.add(layers.Dropout(0.5))  \n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 5 classes.\n",
      "Found 1000 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "train_dir = 'cropped_images/train'\n",
    "valid_dir = 'cropped_images/valid'\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,  \n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True, \n",
    "    brightness_range=[0.5, 1.5],  \n",
    "    fill_mode='nearest' \n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1./255  \n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 664s 5s/step - loss: 1.6064 - accuracy: 0.2175 - val_loss: 0.1512 - val_accuracy: 0.9587\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 655s 5s/step - loss: 1.5997 - accuracy: 0.2345 - val_loss: 0.3228 - val_accuracy: 0.9365\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 681s 5s/step - loss: 1.5922 - accuracy: 0.2407 - val_loss: 0.4637 - val_accuracy: 0.9425\n",
      "Epoch 4/20\n",
      " 66/125 [==============>...............] - ETA: 4:31 - loss: 1.5851 - accuracy: 0.2363"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noble\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('face_recognition_vgg16_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 81s 3s/step\n",
      "Confusion Matrix:\n",
      "[[36 39 39 49 37]\n",
      " [41 33 41 39 46]\n",
      " [35 44 42 37 42]\n",
      " [46 36 40 38 40]\n",
      " [41 47 40 37 35]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Jeswin       0.18      0.18      0.18       200\n",
      "      Joshua       0.17      0.17      0.17       200\n",
      "      Manoah       0.21      0.21      0.21       200\n",
      "      Nikhil       0.19      0.19      0.19       200\n",
      "       Rovin       0.17      0.17      0.17       200\n",
      "\n",
      "    accuracy                           0.18      1000\n",
      "   macro avg       0.18      0.18      0.18      1000\n",
      "weighted avg       0.18      0.18      0.18      1000\n",
      "\n",
      "\n",
      "Accuracy for each class:\n",
      "Jeswin: 0.18\n",
      "Joshua: 0.17\n",
      "Manoah: 0.21\n",
      "Nikhil: 0.19\n",
      "Rovin: 0.17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "class_names = {0: 'Jeswin', 1: 'Joshua', 2: 'Manoah', 3: 'Nikhil', 4: 'Rovin'}\n",
    "\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "y_pred = model.predict(validation_generator)\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "class_report = classification_report(y_true, y_pred_classes, target_names=class_names.values())\n",
    "\n",
    "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"\\nAccuracy for each class:\")\n",
    "for class_name, accuracy in zip(class_names.values(), class_accuracy):\n",
    "    print(f\"{class_name}: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 336ms/step\n",
      "frame_02f44eda-af67-4b11-82a9-f58ffbee6a0e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "frame_039c536c-dcac-4b1d-b68e-8c4320f63fdb.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "frame_04a7afd5-4788-4f48-a0e9-8ace46b7412b.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "frame_04bdce2d-a319-4a18-89b9-f48a5e0e0d2c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "frame_06d69303-de8c-4d71-a1cc-f19c68e73081.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_0b135555-905d-491b-99e8-311fad9089dd.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_0ba353ca-ab3f-4ae4-a739-4fe17ff51309.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "frame_0c5a0a6e-a27b-434c-99be-7f4790f69474.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "frame_0ceb7d52-06e7-461d-b3b9-293090192040.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "frame_0e27a871-3f62-4a58-a7c4-ec53204c8d8c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "frame_0ed72d27-03e7-400d-a661-e66b7b732383.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "frame_0f3ae513-268c-458e-af85-c5d00f58a9c7.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "frame_10a39755-2cf5-4833-8369-7e3cef0aefb9.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_14c98961-7106-4f4f-b89a-e585fa4e5061.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "frame_17fb853f-b995-46fc-933d-2c4dcee694b6.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "frame_18ca906b-10c3-4548-b17d-9814717c7f24.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "frame_18de5324-8d22-45fa-986c-a3cee097a592.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "frame_1a0f6140-4ffb-486b-811c-36058f3b1dff.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_1a8647f9-2c9b-4336-8ef1-89e35b7f6970.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_1c507550-8214-4ae4-9c72-d7c0dc86e0cf.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_1ca176ea-3e54-4ac1-9347-864485be4bbf.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_27be7326-c56a-4359-8463-42a4a9c9a708.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_29fc988a-66f1-4605-9ed1-6dba13e3d1c9.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "frame_2a05801d-ccba-4608-9b58-03e7693c73ec.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "frame_2bc7604d-8581-4566-9eab-eefadcfa43c4.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "frame_2bdefa5b-47a3-492c-a6ba-c3675bced1e4.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "frame_2d530f1a-3b6a-4239-be81-4799be8ec2b3.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "frame_2df49823-4b62-4c75-8b90-60ea08714a1d.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_2e8f6134-567b-41a4-9ebc-ebf7810b8deb.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "frame_2edee006-ff28-4ab9-9650-3a49652cdf4c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "frame_30df6443-1bb5-4d48-bed1-45d6051f3a62.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "frame_32a45454-f696-4b54-98fc-6ff2c0ac47df.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "frame_36f243a3-f6b0-4c2d-a55f-a80fa3a9e37c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_38ff623c-c47c-4858-a9d6-7fe4083fe0de.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_3a6abddf-ff82-4e2f-8e96-f117d657d3da.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "frame_3ba3b357-410b-4cc0-bfa9-fa66c37c5fb4.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "frame_3bfec9b2-c44c-4ce7-8ef5-086912546d36.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "frame_3c83e9d4-68cd-4f61-84e0-378a822f7c9b.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_3cab9cd1-cc4d-4d30-bc17-f9524376e13c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "frame_3cf4d8f1-5cc8-4cf0-a792-2b02e059d94e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_3d4481c2-0b1d-4455-8ea1-0f652fbc9bdb.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "frame_3dc3de35-c363-45d1-a745-db1e12b90696.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_3ebd5695-f127-4d1c-80b9-556050db656c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "frame_45b553b4-4187-4946-8681-13eb4068d54c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "frame_48ca90f4-860e-4cfd-84dc-d6180b3c7573.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "frame_49acaf55-090e-4765-ba9b-f4928e0b28b9.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_49cb4a7a-36e3-47d8-b301-2984ec98e1d6.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_4a396acf-f3cc-4e3e-8e54-722f9179ff23.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_4aa93180-892f-445c-b27c-1e18f911e6b4.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_4c21c142-6b7a-4792-bf55-36bb566a775b.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_4fab99a1-7db8-4684-8af4-64a2aa64eea4.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "frame_50e8ad12-4b51-4da1-8bcb-cf905661fbd5.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "frame_52818969-b4cc-40a0-aa43-9f7e2a108cd0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "frame_56ab4afb-96b4-4a65-8586-11907c05295c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "frame_56c64e5e-0983-484b-a601-e7835e026b29.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "frame_56d99424-df5c-4b0b-a9c5-be391d202b7c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "frame_57246713-00ba-476e-8b93-c7784d942bd0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_58752903-297d-4fa6-91ee-4810eff447d0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_58d2a5d0-13af-4dec-a85b-a2871e099abc.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "frame_5b23f880-40ac-4429-bffb-ed53fde4113f.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_5be2c9e2-85e2-438a-959b-246461acb5ca.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "frame_5dae2545-aa67-4ffc-8939-dd30e851da14.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "frame_5df6985d-7aa4-4b38-af15-7d932036b287.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "frame_5f7f55df-73d5-4a03-b4e8-19181ef987cc.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_5f9e8077-bc24-47e9-823e-8927bb94a0dc.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "frame_5fc4f984-187d-4dcf-8c66-16c7c8f21057.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "frame_68012003-6ac8-44a2-9c8b-67681fd6d828.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_6a2000c8-d9a1-47e3-9d49-dd19d629d704.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "frame_6b2b7110-8251-4411-9fce-0ebf3a997910.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_6bf87a84-65e9-41ed-a9fa-46a5677ba120.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "frame_6c37852c-2675-45b3-aa99-e5e5d669b559.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "frame_6ca990c2-f2ee-49f4-9526-187e1e94d80a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "frame_6d211822-61d8-4195-97f3-cb201393adb0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "frame_6d5ac05f-0446-4fed-891c-eaf36267121c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "frame_6d6543c0-5373-403e-87a3-0c4d07e89ab2.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "frame_6df89295-ed12-4f7a-b03e-f800c71ff2d2.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "frame_6ecba2d8-dafb-495e-9478-9140ada1f8d0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_6fdbb66f-3c62-41d6-a5c5-5981aba207fd.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "frame_73725422-7c92-41ab-8763-087183b9c00a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "frame_75817268-7757-4e22-a7fa-be203832b71f.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "frame_7a60c5e5-54d9-4b61-b1e0-d269c6538909.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "frame_7b070c1e-037a-45a6-b3ce-e74093e5da11.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "frame_7bb59a47-4b16-457a-9d52-796867fff44a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "frame_7bbde025-2011-4cf3-b0e3-6378e26ef93f.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_7ca5b483-4f41-4ccc-9497-095dfa243e91.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_7d8571ec-c382-4a78-8d7a-8475313e9ced.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_8e37d536-1314-449d-9789-39527cbcf5fe.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "frame_8ec852ff-7f05-4d99-a7e6-ee615d6da5ab.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_8fcba10c-69de-4286-b9aa-04bc493300d0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "frame_9337725c-1d14-4a12-9822-69418636d9c5.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "frame_9a01bb31-3516-4532-97ab-b24c8118c329.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "frame_9ad72dc8-138e-49c9-8500-02d400ec0241.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "frame_9b0795a2-b4a7-401c-8009-8b460f4c4361.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "frame_9b52ea83-7762-4c9f-ab56-b7fff9024110.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "frame_9bda9dab-4d0b-45d6-89df-6b234c05dbed.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "frame_9ca380e6-85b6-4bde-aa81-8216945c54f0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "frame_9dd3f23b-61fa-4a56-85eb-6b78fa28730a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_9e54b59c-adf3-4f96-ac57-1b1ad1ef3066.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_9fdf13a0-706d-48ed-a080-5a0373b37b2d.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_a048c992-0ec0-4fad-9376-37b16b57e1ac.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_a09e880f-13bb-4de0-ae9c-8f66684c00f0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "frame_a2ac0238-32f3-47bf-89bc-944341fbd7f0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "frame_a31b78d4-8a51-410d-96c9-a8b0cf0b8994.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "frame_a32828a0-9a98-4aaf-a862-615cd2c4ef9e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "frame_a4283028-91aa-48fc-882c-194a23a5eb4e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "frame_a4a2c287-4f8d-464b-83b4-76dae28fd413.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_a4cb3916-0a66-4af0-9d3a-6610626c01ed.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "frame_a4cddfeb-fdce-426f-b4e1-8176e3c108d0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "frame_a61368ea-b38f-4a02-b8d4-0f200fc8ba25.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_a68299ee-92c8-43fd-b637-f839da8ac4fe.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "frame_a98b5490-4350-44d1-9329-feea76c13bc3.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "frame_a9b49ab6-b8b6-49bf-a740-eb94d0f70652.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_aaee8217-5d2d-4e47-b117-4cad06ce1160.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "frame_ac1fcea8-dd30-48c2-be5f-89856df7311c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "frame_acc8d177-c301-4ffd-af19-13df6fb6117c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "frame_af208a15-d6b8-4c72-8bf9-40abda98417a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "frame_af7655b7-6081-40fa-b9b4-d26daecd416b.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "frame_b07cb35a-ac03-4162-a1ff-acec726f4fbc.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_b1b9aabb-ee11-45ac-a7ca-4131f7552fb1.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_b55523c3-bf67-4eb6-a2c6-2f3c92fc588d.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_b7b6730d-860d-445b-99ed-3e2b3b7619a1.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_b8597647-0333-4b58-86e9-c757294cf400.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_b91a686b-611b-472d-8075-29904b3918d1.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_b99ae5b6-70d4-4474-99e3-5a131abdc244.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_b9c4aeff-7ba0-4e16-9aa0-110f72a3826a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "frame_ba48cf1a-e829-4d83-ae30-c6747775a6c0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "frame_ba8aee48-808a-45ea-9fdb-97fcd3e7a567.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_bb6820d3-7a07-484b-8588-f7669fae3a43.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "frame_bbbd289b-3185-48a6-a27f-9de350e1c623.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "frame_bf69aaf0-dfef-4296-97ff-f6e8da20389a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "frame_c251ffd3-4420-4384-aafc-4eaa7322ca67.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "frame_c496cd43-c356-40a1-9435-e1e46b699ce0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "frame_c5a519f6-732f-4e96-b742-60ed45326e88.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "frame_c67c6ece-c764-42f1-8cc5-f802ff74e2cc.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "frame_c7a798bc-4632-416e-84b9-05f7ae996741.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_c7e9636a-5ce8-475b-b987-5f4e3db0c154.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "frame_ca116821-b107-45f6-80ef-85657ddffd03.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "frame_ca599b6d-84fa-4465-868b-b3d993388ca1.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "frame_ce44a4f0-fcb2-476d-a29a-f5980cce7e17.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "frame_ce866eed-881e-450e-aafa-421fddb83fa9.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "frame_cf8f55ff-845a-4094-9d1d-def662a45e9e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "frame_d027b122-4522-4eea-935a-f80267a8e6fb.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_d2bcc124-6489-4aef-a543-10e6fc580b4e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_d2c850ac-5751-47b1-aad2-05626921a087.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "frame_d4330b8e-2f15-446c-b31d-c65949e4e189.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "frame_d442b292-219b-4c83-8658-d0924b4a22e2.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "frame_d4b24dd6-9ef4-457e-a2c7-00c8cba16962.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "frame_d593d903-9dd3-4c6e-8fa9-639138ca365e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "frame_d5baeaa1-68cc-4fca-9deb-cda4ee8521e9.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "frame_d5fe81ae-78f0-4c69-9af8-c1bbca35f606.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "frame_d6257d92-fab8-4598-9bd0-37c6207b3199.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "frame_d634ded5-cc39-4fc7-8ac2-2c3b73c0a464.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "frame_d64243f1-02af-4bf3-a816-9f08fa6467c1.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "frame_d66c5840-2337-404f-98b5-76ac8a4d3ea3.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "frame_d715b1b5-fd4a-405d-984b-d9b282b71fe2.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "frame_d73e7576-4b7a-4584-a351-30aaf6a262ba.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "frame_d788cc5d-eb48-4ea6-9335-597c994cd5be.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_d7afbaac-0499-4c93-bd89-db3b42be17cd.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "frame_d8743aac-480b-49a8-bc21-420f1f94bf60.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "frame_d9b1e542-ccc4-4932-91c8-97637d20d518.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "frame_db8f5dab-4586-4064-b86d-2f30139831a8.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "frame_df7df2e4-c3db-4ee7-8bad-4272d9ed5475.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "frame_e1271c28-7118-48ed-935f-c7d29c947bb2.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "frame_e1379c86-7e67-42e7-9b59-81475859fc69.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "frame_e1ae38cd-7a78-4d2d-b370-ddc89b8b6ab6.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "frame_e1cb686b-cf0a-44b7-88ab-586107cd4b52.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_e2b797ca-804d-499f-ba0b-447c62628640.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "frame_e382ab00-cb9e-4a3a-a36b-f52a64edbd53.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "frame_e3b3ab27-3d2a-4262-80d9-528ae13de2fa.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_e5170637-9628-4f7f-8150-f20e84cacc7a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "frame_e5e5e5fe-42ff-43de-b625-35964552e51c.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "frame_e6049d11-692b-4724-8d87-8aa0aa8543cd.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "frame_e6daa84b-11aa-48f6-a4bf-a01893f9c5e0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "frame_e77a129a-abd7-4e25-93da-606512e0a24e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "frame_ea037a3c-74b0-49fc-840f-f7f42e73854d.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_eaa4084b-a48d-4ab3-9215-a1d8f7801d34.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "frame_ebdbfb12-6b03-4baf-b4a1-6dc1ba2c0783.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "frame_ec6232c6-2fca-4358-b7f2-81066d16fe99.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "frame_ee64586c-f650-4df4-89ff-e3a6192c8dc3.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_f1dd78b1-fded-48c8-ba38-ddc1f413344e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "frame_f20f6435-b962-4add-a513-c864c46fb493.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_f4e10e92-5069-456a-8e21-7479de546fc0.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_f5acceb0-acfe-43ad-b926-f891a987e947.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "frame_f76ee093-1d2a-4389-b43b-e5443caa89df.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "frame_f7bd0dea-f20d-4219-906e-191e75ddbb5f.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_f7fd9730-4f2a-4ea4-ae32-4ced8e5601e9.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_f838b823-2d19-4a6d-b2a4-e7736ed30299.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "frame_f8d57eda-6380-4592-8b5a-cbc4c0856016.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_f8f368a3-03d9-45f3-9097-b6b51dddad0a.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "frame_faea1e8d-4944-4f80-8661-c4e3fa030ee6.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "frame_fb01a864-2383-4ae5-a7a4-dd0249c708da.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_fb082130-c4e4-4dbc-a6ab-79743c1f9227.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "frame_fd40df36-eac0-4424-8773-108abb44ac7b.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "frame_fd9d3ae9-01a7-4fb9-a25c-fda44a8a083f.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "frame_fdb2083f-06e6-4be5-b5e0-d61db35d1b9e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "frame_fe664f7c-479d-4f7c-aeae-33629838bc37.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_fe93269c-4c3c-4203-9a86-be934d367049.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "frame_ff1d46a8-247b-46cd-8cc7-ea4ca5ac2b88.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "frame_ffa44999-b9fb-41d8-8e96-b05cbfab877e.jpg: Predicted class - Manoah\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "frame_ffefb0f5-ee96-499c-98d4-35adbe23e44c.jpg: Predicted class - Manoah\n",
      "\n",
      "Counts of predictions for each class:\n",
      "Jeswin: 0\n",
      "Joshua: 0\n",
      "Manoah: 200\n",
      "Nikhil: 0\n",
      "Rovin: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('face_recognition_vgg16_model_1.h5')\n",
    "\n",
    "class_names = {0: 'Jeswin', 1: 'Joshua', 2: 'Manoah', 3: 'Nikhil', 4: 'Rovin'}\n",
    "\n",
    "images_directory = 'cropped_images/valid/Manoah'\n",
    "\n",
    "class_counts = {class_name: 0 for class_name in class_names.values()}\n",
    "\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        \n",
    "        img = cv2.imread(image_path)\n",
    "        img_resized = cv2.resize(img, (224, 224))  \n",
    "        img_array = image.img_to_array(img_resized)  \n",
    "        img_array_expanded_dims = np.expand_dims(img_array, axis=0)  \n",
    "        img_preprocessed = img_array_expanded_dims / 255.0  \n",
    "        \n",
    "        predictions = model.predict(img_preprocessed)\n",
    "        \n",
    "        predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "        predicted_class_name = class_names[predicted_class_index]\n",
    "        \n",
    "        class_counts[predicted_class_name] += 1\n",
    "        \n",
    "        print(f\"{filename}: Predicted class - {predicted_class_name}\")\n",
    "\n",
    "print(\"\\nCounts of predictions for each class:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"{class_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "Total occurrences per emotion in the directory: {'Anger': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 1, 'Nikhil': 0, 'Rovin': 0}, 'Angry': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 0, 'Nikhil': 0, 'Rovin': 0}, 'Disgust': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 24, 'Nikhil': 0, 'Rovin': 0}, 'Disgusted': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 0, 'Nikhil': 0, 'Rovin': 0}, 'Fear': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 0, 'Nikhil': 0, 'Rovin': 0}, 'Fearful': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 4, 'Nikhil': 0, 'Rovin': 0}, 'Happiness': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 18, 'Nikhil': 0, 'Rovin': 0}, 'Happy': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 36, 'Nikhil': 0, 'Rovin': 0}, 'Neutral': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 7, 'Nikhil': 0, 'Rovin': 0}, 'Sad': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 0, 'Nikhil': 0, 'Rovin': 0}, 'Sadness': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 36, 'Nikhil': 0, 'Rovin': 0}, 'Surprise': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 11, 'Nikhil': 0, 'Rovin': 0}, 'Surprised': {'Jeswin': 0, 'Joshua': 0, 'Manoah': 0, 'Nikhil': 0, 'Rovin': 0}}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "model = load_model('face_recognition_vgg16_model_1.h5')\n",
    "\n",
    "class_names = {0: 'Jeswin', 1: 'Joshua', 2: 'Manoah', 3: 'Nikhil', 4: 'Rovin'}\n",
    "\n",
    "emotions_occurrences = {}\n",
    "\n",
    "main_directory = 'parent'\n",
    "\n",
    "for subdir in os.listdir(main_directory):\n",
    "    subdir_path = os.path.join(main_directory, subdir)\n",
    "    if os.path.isdir(subdir_path):  \n",
    "        emotions_occurrences[subdir] = {'Jeswin': 0, 'Joshua': 0, 'Manoah': 0, 'Nikhil': 0, 'Rovin': 0}\n",
    "        \n",
    "        for filename in os.listdir(subdir_path):\n",
    "            if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  \n",
    "                img_path = os.path.join(subdir_path, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img_resized = cv2.resize(img, (224, 224))\n",
    "                    img_array = image.img_to_array(img_resized)\n",
    "                    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "                    img_preprocessed = img_array_expanded_dims / 255. \n",
    "\n",
    "                    prediction = model.predict(img_preprocessed)\n",
    "                    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "                    predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "                    emotions_occurrences[subdir][predicted_class_name] += 1\n",
    "\n",
    "print(\"Total occurrences per emotion in the directory:\", emotions_occurrences)\n",
    "\n",
    "with open('emotion_occurrences.json', 'w') as json_file:\n",
    "    json.dump(emotions_occurrences, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 297ms/step\n",
      "Rovin\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('face_recognition_vgg16_model_1.h5')\n",
    "class_names = {0: 'Jeswin', 1: 'Joshua', 2: 'Manoah', 3: 'Nikhil', 4: 'Rovin'}\n",
    "\n",
    "img = cv2.imread('parent/Sad/face_501_269.jpg')\n",
    "img_resized = cv2.resize(img, (224, 224))\n",
    "img_array = image.img_to_array(img_resized)\n",
    "img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "img_preprocessed = img_array_expanded_dims / 255.  \n",
    "\n",
    "prediction = model.predict(img_preprocessed)\n",
    "predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "predicted_class_name = class_names[predicted_class_index]\n",
    "print(predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\noble\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\noble\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\noble\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Found 1000 images belonging to 5 classes.\n",
      "32/32 [==============================] - 165s 5s/step\n",
      "Confusion Matrix:\n",
      "[[198   0   2   0   0]\n",
      " [  1 199   0   0   0]\n",
      " [  0   0 200   0   0]\n",
      " [  0   0   0 200   0]\n",
      " [  0   0   0   0 200]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Jeswin       0.99      0.99      0.99       200\n",
      "      Joshua       1.00      0.99      1.00       200\n",
      "      Manoah       0.99      1.00      1.00       200\n",
      "      Nikhil       1.00      1.00      1.00       200\n",
      "       Rovin       1.00      1.00      1.00       200\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "\n",
      "Accuracy for each class:\n",
      "Jeswin: 0.99\n",
      "Joshua: 0.99\n",
      "Manoah: 1.00\n",
      "Nikhil: 1.00\n",
      "Rovin: 1.00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Replace 'path/to/your/model.h5' with the actual path to your saved model\n",
    "model = load_model('face_recognition_vgg16_model_1.h5')\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Assuming you have a validation data directory and preprocessing steps defined\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)  # Example preprocessing step\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'cropped_images/valid',\n",
    "    target_size=(224, 224),  # Assuming this is the target size you trained with\n",
    "    batch_size=32,  # Or your batch size\n",
    "    class_mode='categorical',  # Assuming you're doing categorical classification\n",
    "    shuffle=False  # Important for evaluation to not shuffle the data\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Your class names\n",
    "class_names = {0: 'Jeswin', 1: 'Joshua', 2: 'Manoah', 3: 'Nikhil', 4: 'Rovin'}\n",
    "\n",
    "# Obtain true labels and predictions\n",
    "y_true = validation_generator.classes\n",
    "y_pred = model.predict(validation_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Generate confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "class_report = classification_report(y_true, y_pred_classes, target_names=class_names.values())\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Print the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"\\nAccuracy for each class:\")\n",
    "for class_name, accuracy in zip(class_names.values(), class_accuracy):\n",
    "    print(f\"{class_name}: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
